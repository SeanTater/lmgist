project:
  name: embedding-to-text-decoding
  run_name: stage-a-ministral-3b-k16
  seed: 42

paths:
  data_dir: data
  embeddings_dir: data/embeddings
  outputs_dir: outputs

hardware:
  device: cuda
  mixed_precision: bf16

dataset:
  name: ms_marco
  config: v2.1
  split: train
  validation_split: validation
  test_split: test
  text_field: query
  id_field: query_id
  max_tokens: 512
  train_limit: null # 1000 in mini-set
  validation_limit: 2000 # 200 in mini-set
  test_limit: 2000 # 200 in mini-set
  shuffle_seed: 42

embeddings:
  model: intfloat/e5-base-v2
  pooling: max
  batch_size: 64
  precompute: auto
  storage_format: npy
  max_precompute_gb: 250

model:
  decoder_model: ministral/Ministral-3B-Instruct
  prefix_tokens: 16
  adapter_hidden_dim: 2048
  adapter_layers: 2
  dropout: 0.1

prompt:
  system: "You are a helpful assistant."
  user_prefix: "Repeat the following text:"

training:
  epochs: 1
  batch_size: 8
  grad_accum_steps: 4
  learning_rate: 0.0002
  weight_decay: 0.01
  warmup_steps: 200
  max_grad_norm: 1.0
  log_every_steps: 1000
  save_every_steps: 500
  eval_every_steps: 500

evaluation:
  split: validation
  max_samples: 200
  batch_size: 4
  max_new_tokens: 128
  temperature: 0.7
  top_p: 0.9
  adapter_checkpoint: outputs/adapter-final.pt

