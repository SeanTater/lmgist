project:
  name: embedding-to-text-decoding
  run_name: stage-b-llama-3.2-1b-k1
  seed: 42

paths:
  data_dir: data
  embeddings_dir: data/embeddings
  outputs_dir: outputs/k1

hardware:
  device: cuda

dataset:
  name: ms_marco
  config: v2.1
  split: train
  validation_split: validation
  test_split: test
  text_field: query
  id_field: query_id
  max_tokens: 512
  train_limit: null # 1000 in mini-set
  validation_limit: 2000 # 200 in mini-set
  test_limit: 2000 # 200 in mini-set
  shuffle_seed: 42

embeddings:
  model: intfloat/e5-base-v2
  batch_size: 64 # embedding batch size for precompute/inference
  precompute: auto # auto|true|false; auto enables if estimated storage <= max_precompute_gb
  max_precompute_gb: 250 # auto precompute cap (GB) for embeddings storage

model:
  decoder_model: meta-llama/Llama-3.2-1B-Instruct
  prefix_tokens: 1
  adapter_hidden_dim: 2048
  adapter_layers: 2
  dropout: 0.1

lora:
  enabled: false
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj

prompt:
  system: "You are a helpful assistant."
  user_prefix: "Reconstruct the original text exactly."

training:
  epochs: 8
  batch_size: 8
  grad_accum_steps: 4 # gradient accumulation steps to simulate larger batch
  learning_rate: 0.0002
  weight_decay: 0.01
  warmup_steps: 200 # linear warmup steps
  max_grad_norm: 1.0
  log_every_steps: 2500 # training log interval (steps)
  save_every_steps: 25000 # checkpoint save interval (steps)
  eval_every_steps: 2500 # eval interval (steps) for selection task
  adapter_checkpoint: null # path to adapter checkpoint to resume from

evaluation:
  split: validation
  max_samples: 500 # max eval samples; use null for full split
  batch_size: 4
  max_new_tokens: 128
  temperature: 0.7
  top_p: 0.9
  do_sample: false
  adapter_checkpoint: outputs/k1/adapter-final.pt
  length_buckets: 8
