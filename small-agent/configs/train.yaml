# LoRA SFT training config
model_name: "Qwen/Qwen3-4B"
lora:
  rank: 32
  alpha: 64
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  dropout: 0.05

training:
  epochs: 1
  lr: 3.0e-5
  batch_size: 2
  grad_accum_steps: 16
  warmup_ratio: 0.1
  max_seq_len: 2048
  weight_decay: 0.01
  bf16: true

output_dir: "checkpoints/sft"
